//! NPC Dialog Engine
//!
//! Generates NPC dialog based on configuration.
//! Can use rule-based (hardcoded) or LLM-powered responses.
//!
//! # Architecture
//! ```text
//! NpcEngine
//! ├── provider: Provider      (shared LLM client)
//! ├── cache: ResponseCache    (response caching)
//! ├── config: GameConfig      (class definitions)
//! └── conversations: HashMap<NpcId, ConversationHistory>
//! ```
//!
//! # Flow
//! 1. Get NPC class config (engine type, persona)
//! 2. If engine == Rule: return static dialog
//! 3. If engine == Llm:
//!    a. Check cache
//!    b. Build prompt with persona + context
//!    c. Call LLM
//!    d. Cache response
//! 4. If engine == Hybrid: Try LLM, fallback to rule

use std::collections::HashMap;
use std::time::Instant;
use anyhow::Result;
use rand::seq::SliceRandom;

use crate::llm::{LlmMessage, LlmProvider};
use super::cache::ResponseCache;
use super::config::GameConfig;
use super::context::GameContext;
use super::traits::EngineType;

/// Conversation history per NPC instance
///
/// Tracks the back-and-forth between player and NPC.
/// Limited to prevent token bloat in LLM calls.
pub struct ConversationHistory {
    /// Message history (user + assistant exchanges)
    pub messages: Vec<LlmMessage>,
    /// When the last message was sent
    pub last_interaction: Instant,
}

impl ConversationHistory {
    /// Maximum messages to keep in history
    const MAX_MESSAGES: usize = 10;
    
    /// Create empty conversation history
    pub fn new() -> Self {
        Self {
            messages: Vec::new(),
            last_interaction: Instant::now(),
        }
    }
    
    /// Add a message to the history
    ///
    /// Removes oldest messages if exceeding MAX_MESSAGES
    pub fn add_message(&mut self, role: &str, content: String) {
        if self.messages.len() >= Self::MAX_MESSAGES {
            self.messages.remove(0);
        }
        self.messages.push(LlmMessage {
            role: role.into(),
            content,
        });
        self.last_interaction = Instant::now();
    }
    
    /// Clear conversation history
    pub fn clear(&mut self) {
        self.messages.clear();
    }
}

impl Default for ConversationHistory {
    fn default() -> Self {
        Self::new()
    }
}

/// Input for NPC dialog generation
pub struct NpcInput {
    /// Unique NPC identifier (for conversation tracking)
    pub npc_id: usize,
    /// NPC class name (e.g., "recruiter", "barista")
    pub npc_class: String,
    /// NPC display name
    pub npc_name: String,
    /// Optional player message (for conversation mode)
    pub player_message: Option<String>,
}

/// Output from NPC dialog generation
pub struct NpcOutput {
    /// The dialog text to display
    pub text: String,
    /// Whether this was generated by LLM or rule engine
    pub from_llm: bool,
}

/// NPC Dialog Engine
///
/// Manages dialog generation for all NPCs in the game.
pub struct NpcEngine {
    /// LLM provider for dynamic responses
    provider: crate::llm::Provider,
    /// Response cache
    cache: ResponseCache,
    /// Game configuration
    config: GameConfig,
    /// Conversation history per NPC
    conversations: HashMap<usize, ConversationHistory>,
}

impl NpcEngine {
    /// Create a new NPC engine
    ///
    /// # Arguments
    /// * `config` - Game configuration
    ///
    /// # Errors
    /// Returns error if LLM provider creation fails
    pub fn new(config: GameConfig) -> Result<Self> {
        let provider = crate::llm::create_provider(&crate::llm::LlmConfig {
            provider: config.llm.provider.clone(),
            model: config.llm.model.clone(),
        })?;
        
        Ok(Self {
            provider,
            cache: ResponseCache::new(),
            config,
            conversations: HashMap::new(),
        })
    }
    
    /// Create engine with mock provider (for testing)
    pub fn with_mock(config: GameConfig, response: &str) -> Self {
        Self {
            provider: crate::llm::Provider::Mock(
                crate::llm::MockProvider::new(response)
            ),
            cache: ResponseCache::new(),
            config,
            conversations: HashMap::new(),
        }
    }
    
    /// Get the engine type for an NPC class
    pub fn get_engine_type(&self, npc_class: &str) -> EngineType {
        self.config.get_npc_engine(npc_class)
    }
    
    /// Get dialog for an NPC
    ///
    /// # Arguments
    /// * `input` - NPC identification and optional player message
    /// * `context` - Current game state
    ///
    /// # Returns
    /// Dialog text and metadata
    pub async fn get_dialog(
        &mut self,
        input: &NpcInput,
        context: &GameContext,
    ) -> Result<NpcOutput> {
        let engine_type = self.config.get_npc_engine(&input.npc_class);
        
        let (text, from_llm) = match engine_type {
            EngineType::Rule => (self.rule_dialog(&input.npc_class)?, false),
            EngineType::Llm => (self.llm_dialog(input, context).await?, true),
            EngineType::Hybrid => {
                match self.llm_dialog(input, context).await {
                    Ok(text) => (text, true),
                    Err(_) => (self.rule_dialog(&input.npc_class)?, false),
                }
            }
        };
        
        Ok(NpcOutput { text, from_llm })
    }
    
    /// Get rule-based dialog for an NPC class
    fn rule_dialog(&self, npc_class: &str) -> Result<String> {
        let dialog = self.config.get_npc_fallback_dialog(npc_class)
            .ok_or_else(|| anyhow::anyhow!("No fallback dialog for class: {}", npc_class))?;
        
        // Pick a random line
        let text = dialog
            .choose(&mut rand::thread_rng())
            .cloned()
            .unwrap_or_else(|| "Hello!".to_string());
        
        Ok(text)
    }
    
    /// Get LLM-powered dialog
    async fn llm_dialog(
        &mut self,
        input: &NpcInput,
        context: &GameContext,
    ) -> Result<String> {
        // Check cache first
        let cache_key = ResponseCache::make_key(
            &format!("npc_{}", input.npc_class),
            &input.player_message.clone().unwrap_or_default(),
            context,
        );
        
        if let Some(cached) = self.cache.get(&cache_key) {
            return Ok(cached);
        }
        
        // Build system prompt
        let persona = self.config.get_npc_persona(&input.npc_class)
            .unwrap_or("You are a friendly NPC.");
        
        let system = format!(
            "{}\n\n{}\n\nYour name is {}. Respond naturally.",
            persona,
            context.to_prompt_section(),
            input.npc_name,
        );
        
        // Get or create conversation history
        let history = self.conversations
            .entry(input.npc_id)
            .or_insert_with(ConversationHistory::new);
        
        // Build messages
        let mut messages = history.messages.clone();
        
        if let Some(player_msg) = &input.player_message {
            messages.push(LlmMessage::user(player_msg.clone()));
        } else {
            // First interaction - use a greeting prompt
            messages.push(LlmMessage::user("Hello!".to_string()));
        }
        
        // Call LLM
        let response = self.provider.complete(&system, messages).await?;
        
        // Update conversation history
        if let Some(player_msg) = &input.player_message {
            let history = self.conversations.get_mut(&input.npc_id).unwrap();
            history.add_message("user", player_msg.clone());
            history.add_message("assistant", response.clone());
        }
        
        // Cache the response
        self.cache.set(cache_key, response.clone());
        
        Ok(response)
    }
    
    /// Clear conversation history for an NPC
    pub fn clear_conversation(&mut self, npc_id: usize) {
        if let Some(history) = self.conversations.get_mut(&npc_id) {
            history.clear();
        }
    }
    
    /// Clear all conversation histories
    pub fn clear_all_conversations(&mut self) {
        self.conversations.clear();
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_conversation_history_limit() {
        let mut history = ConversationHistory::new();
        
        for i in 0..15 {
            history.add_message("user", format!("Message {}", i));
        }
        
        assert_eq!(history.messages.len(), ConversationHistory::MAX_MESSAGES);
    }
    
    #[tokio::test]
    async fn test_rule_dialog() {
        let config = GameConfig::load().unwrap();
        let mut engine = NpcEngine::with_mock(config, "Test response");
        
        let input = NpcInput {
            npc_id: 1,
            npc_class: "barista".to_string(),
            npc_name: "Morgan".to_string(),
            player_message: None,
        };
        
        let output = engine.get_dialog(&input, &GameContext::empty()).await.unwrap();
        
        // Barista is rule-based, so should get fallback dialog
        assert!(!output.from_llm);
    }
}
